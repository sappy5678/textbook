
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>17.5. Approximating the Empirical Probability Distribution &#8212; Principles and Techniques of Data Science</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.2d2078699c18a0efb88233928e1cf6ed.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.be0a4a0c39cd630af62a2fcf693f3f06.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="17.6. Fitting a Logistic Model" href="classification_sgd.html" />
    <link rel="prev" title="17.4. Using Logistic Regression" href="classification_log_reg.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  
  <h1 class="site-logo" id="site-title">Principles and Techniques of Data Science</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Frontmatter
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../prereqs.html">
   Prerequisites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation.html">
   Notation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Chapters
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../01/lifecycle_intro.html">
   1. The Data Science Lifecycle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02/design_intro.html">
   2. Data Design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../03/pandas_intro.html">
   3. Working with Tabular Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04/eda_intro.html">
   4. Exploratory Data Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05/cleaning_intro.html">
   5. Data Cleaning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../06/viz_intro.html">
   6. Data Visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../07/web_intro.html">
   7. Web Technologies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../08/text_intro.html">
   8. Working with Text
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../09/sql_intro.html">
   9. Relational Databases and SQL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../10/modeling_intro.html">
   10. Modeling and Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../11/gradient_descent.html">
   11. Gradient Descent and Numerical Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../12/prob_and_gen.html">
   12. Probability and Generalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../13/linear_models.html">
   13. Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../14/feature_engineering.html">
   14. Feature Engineering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../15/bias_intro.html">
   15. The Bias-Variance Tradeoff
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../16/reg_intro.html">
   16. Regularization
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="classification_intro.html">
   17. Classification
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="classification_prob.html">
     17.1. Regression on Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_log_model.html">
     17.2. The Logistic Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_cost.html">
     17.3. A Loss Function for the Logistic Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_log_reg.html">
     17.4. Using Logistic Regression
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     17.5. Approximating the Empirical Probability Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_sgd.html">
     17.6. Fitting a Logistic Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_sensitivity_specificity.html">
     17.7. Evaluating Logistic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_multiclass.html">
     17.8. Multiclass Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../18/hyp_intro.html">
   18. Statistical Inference
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendices
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../19/vector_space_review.html">
   Vector Space Review
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../20/ref_intro.html">
   Reference Tables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../21/contributors.html">
   Contributors
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/ch/17/classification_cost_justification.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/ch/17/classification_cost_justification.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-average-kl-divergence">
   17.5.1. Defining Average KL Divergence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deriving-cross-entropy-loss-from-kl-divergence">
   17.5.2. Deriving Cross-Entropy Loss from KL Divergence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#statistical-justification-for-cross-entropy-loss">
   17.5.3. Statistical justification for Cross-Entropy Loss
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   17.5.4. Summary
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell tag_hide_input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="c1"># Ignore numpy dtype warnings. These warnings are caused by an interaction</span>
<span class="c1"># between numpy and Cython and can be safely ignored.</span>
<span class="c1"># Reference: https://stackoverflow.com/a/40846742</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;numpy.dtype size changed&quot;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;numpy.ufunc size changed&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span><span class="p">,</span> <span class="n">interact_manual</span>
<span class="kn">import</span> <span class="nn">nbinteract</span> <span class="k">as</span> <span class="nn">nbi</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;talk&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_rows</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_columns</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># This option stops scientific notation for pandas</span>
<span class="c1"># pd.set_option(&#39;display.float_format&#39;, &#39;{:.2f}&#39;.format)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="approximating-the-empirical-probability-distribution">
<h1><span class="section-number">17.5. </span>Approximating the Empirical Probability Distribution<a class="headerlink" href="#approximating-the-empirical-probability-distribution" title="Permalink to this headline">¶</a></h1>
<p>In this section, we introduce <strong>KL divergence</strong> and demonstrate how minimizing average KL divergence in binary classification is equivalent to minimizing average cross-entropy loss.</p>
<p>Since logistic regression outputs probabilities, a logistic model produces a certain type of probability distribution. Specifically, based on optimal parameters <span class="math notranslate nohighlight">\( \hat{\boldsymbol{\theta}} \)</span>, it estimates the probability that the label <span class="math notranslate nohighlight">\( y \)</span> is <span class="math notranslate nohighlight">\( 1 \)</span> for an example input <span class="math notranslate nohighlight">\( \textbf{x} \)</span>.</p>
<p>For example, suppose that <span class="math notranslate nohighlight">\( x \)</span> is a scalar recording the forecasted chance of rain for the day and <span class="math notranslate nohighlight">\( y = 1 \)</span> means that Mr. Doe takes his umbrella with him to work. A logistic model with scalar parameter <span class="math notranslate nohighlight">\( \hat{\theta} \)</span> predicts the probability that Mr. Doe takes his umbrella given a forecasted chance of rain: <span class="math notranslate nohighlight">\( \hat{P_\theta}(y = 1 | x) \)</span>.</p>
<p>Collecting data on Mr. Doe’s umbrella usage provides us with a method of constructing an empirical probability distribution <span class="math notranslate nohighlight">\( P(y = 1 | x) \)</span>. For example, if there were five days where the chance of rain <span class="math notranslate nohighlight">\( x = 0.60 \)</span> and Mr. Doe only took his umbrella to work once, <span class="math notranslate nohighlight">\( P(y = 1 | x = 0.60) = 0.20 \)</span>. We can compute a similar probability distribution for each value of <span class="math notranslate nohighlight">\( x \)</span> that appears in our data. Naturally, after fitting a logistic model we would like the distribution predicted by the model to be as close as possible to the empirical distribution from the dataset. That is, for all values of <span class="math notranslate nohighlight">\( x \)</span> that appear in our data, we want:</p>
<div class="math notranslate nohighlight">
\[ \hat{P_\theta}(y = 1 | x) \approx P(y = 1 | x) \]</div>
<p>One commonly used metric to determine the “closeness” of two probability distributions is the Kullback–Leibler divergence, or KL divergence, which has its roots in information theory.</p>
<div class="section" id="defining-average-kl-divergence">
<h2><span class="section-number">17.5.1. </span>Defining Average KL Divergence<a class="headerlink" href="#defining-average-kl-divergence" title="Permalink to this headline">¶</a></h2>
<p>KL divergence quantifies the difference between the probability distribution <span class="math notranslate nohighlight">\(\hat{P_\boldsymbol{\theta}}\)</span> computed by our logistic model with parameters <span class="math notranslate nohighlight">\( \boldsymbol{\theta} \)</span> and the actual distribution <span class="math notranslate nohighlight">\( P \)</span> based on the dataset. Intuitively, it calculates how imprecisely the logistic model estimates the distribution of labels in data.</p>
<p>The KL divergence for binary classification between two distributions <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(\hat{P_\boldsymbol{\theta}}\)</span> for a single data point <span class="math notranslate nohighlight">\((\textbf{x}, y)\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[D(P || \hat{P_\boldsymbol{\theta}}) = P(y = 0 | \textbf{x}) \ln \left(\frac{P(y = 0 | \textbf{x})}{\hat{P_\boldsymbol{\theta}}(y = 0 | \textbf{x})}\right) + P(y = 1 | \textbf{x}) \ln \left(\frac{P(y = 1 | \textbf{x})}{\hat{P_\boldsymbol{\theta}}(y = 1 | \textbf{x})}\right)\]</div>
<p>KL divergence is not symmetric, i.e., the divergence of <span class="math notranslate nohighlight">\(\hat{P_\boldsymbol{\theta}}\)</span> from <span class="math notranslate nohighlight">\(P\)</span> is not the same as the divergence of <span class="math notranslate nohighlight">\(P\)</span> from <span class="math notranslate nohighlight">\(\hat{P_\boldsymbol{\theta}}\)</span>: $<span class="math notranslate nohighlight">\(D(P || \hat{P_\boldsymbol{\theta}}) \neq D(\hat{P_\boldsymbol{\theta}} || P)\)</span>$</p>
<p>Since our goal is to use <span class="math notranslate nohighlight">\(\hat{P_\boldsymbol{\theta}}\)</span> to approximate <span class="math notranslate nohighlight">\(P\)</span>, we are concerned with <span class="math notranslate nohighlight">\( D(P || \hat{P_\boldsymbol{\theta}}) \)</span>.</p>
<p>The best <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> values, which we denote as <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}\)</span>, minimize the average KL divergence of the entire dataset of <span class="math notranslate nohighlight">\(n\)</span> points:</p>
<div class="math notranslate nohighlight">
\[ \text{Average KL Divergence} = \frac{1}{n} \sum_{i=1}^{n} \left(P(y_i = 0 | \textbf{X}_i) \ln \left(\frac{P(y_i = 0 | \textbf{X}_i)}{\hat{P_\boldsymbol{\theta}}(y_i = 0 | \textbf{X}_i)}\right) + P(y_i = 1 | \textbf{X}_i) \ln \left(\frac{P(y_i = 1 | \textbf{X}_i)}{\hat{P_\boldsymbol{\theta}}(y_i = 1 | \textbf{X}_i)}\right)\right)\]</div>
<div class="math notranslate nohighlight">
\[ \hat{\boldsymbol{\theta}} = \displaystyle\arg \min_{\substack{\boldsymbol{\theta}}} (\text{Average KL Divergence}) \]</div>
<p>In the above equation, the <span class="math notranslate nohighlight">\(i^{\text{th}}\)</span> data point is represented as (<span class="math notranslate nohighlight">\( \textbf{X}_i \)</span>, <span class="math notranslate nohighlight">\( y_i \)</span>) where <span class="math notranslate nohighlight">\( \textbf{X}_i \)</span> is the <span class="math notranslate nohighlight">\(i^{\text{th}}\)</span> row of the <span class="math notranslate nohighlight">\(n \times p\)</span> data matrix <span class="math notranslate nohighlight">\(\textbf{X}\)</span> and <span class="math notranslate nohighlight">\( y_i \)</span> is the observed outcome.</p>
<p>KL divergence does not penalize mismatch for rare events with respect to <span class="math notranslate nohighlight">\(P\)</span>. If the model predicts a high probability for an event that is actually rare, then both <span class="math notranslate nohighlight">\(P(k)\)</span> and <span class="math notranslate nohighlight">\(\ln \left(\frac{P(k)}{\hat{P_\boldsymbol{\theta}}(k)}\right)\)</span> are low so the divergence is also low. However, if the model predicts a low probability for an event that is actually common, then the divergence is high. We can deduce that a logistic model that accurately predicts common events has a lower divergence from <span class="math notranslate nohighlight">\(P\)</span> than does a model that accurately predicts rare events but varies widely on common events.</p>
</div>
<div class="section" id="deriving-cross-entropy-loss-from-kl-divergence">
<h2><span class="section-number">17.5.2. </span>Deriving Cross-Entropy Loss from KL Divergence<a class="headerlink" href="#deriving-cross-entropy-loss-from-kl-divergence" title="Permalink to this headline">¶</a></h2>
<p>The structure of the above average KL divergence equation contains some surface similarities with cross-entropy loss. We will now show with some algebraic manipulation that minimizing average KL divergence is in fact equivalent to minimizing average cross-entropy loss.</p>
<p>Using properties of logarithms, we can rewrite the weighted log ratio:
$<span class="math notranslate nohighlight">\(P(y_i = k | \textbf{X}_i) \ln \left(\frac{P(y_i = k | \textbf{X}_i)}{\hat{P_\boldsymbol{\theta}}(y_i = k | \textbf{X}_i)}\right) = P(y_i = k | \textbf{X}_i) \ln P(y_i = k | \textbf{X}_i) - P(y_i = k | \textbf{X}_i) \ln \hat{P_\boldsymbol{\theta}}(y_i = k | \textbf{X}_i)\)</span>$</p>
<p>Note that since the first term doesn’t depend on <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, it doesn’t affect <span class="math notranslate nohighlight">\(\displaystyle\arg \min_{\substack{\boldsymbol{\theta}}}\)</span> and can be removed from the equation. The resulting expression is the cross-entropy loss of the model <span class="math notranslate nohighlight">\(\hat{P_\boldsymbol{\theta}}\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \text{Average Cross-Entropy Loss} = \frac{1}{n} \sum_{i=1}^{n} - P(y_i = 0 | \textbf{X}_i) \ln \hat{P_\theta}(y_i = 0 | \textbf{X}_i) - P(y_i = 1 | \textbf{X}_i) \ln \hat{P_\theta}(y_i = 1 | \textbf{X}_i)\]</div>
<div class="math notranslate nohighlight">
\[ \hat{\boldsymbol{\theta}} = \displaystyle\arg \min_{\substack{\theta}} (\text{Average Cross-Entropy Loss}) \]</div>
<p>Since the label <span class="math notranslate nohighlight">\(y_i\)</span> is a known value, the probability that <span class="math notranslate nohighlight">\(y_i = 1\)</span>, <span class="math notranslate nohighlight">\(P(y_i = 1 | \textbf{X}_i)\)</span>, is equal to <span class="math notranslate nohighlight">\(y_i\)</span> and <span class="math notranslate nohighlight">\(P(y_i = 0 | \textbf{X}_i)\)</span> is equal to <span class="math notranslate nohighlight">\(1 - y_i\)</span>. The model’s probability distribution <span class="math notranslate nohighlight">\(\hat{P_\boldsymbol{\theta}}\)</span> is given by the output of the sigmoid function discussed in the previous two sections. After making these substitutions, we arrive at the average cross-entropy loss equation:</p>
<div class="math notranslate nohighlight">
\[ \text{Average Cross-Entropy Loss} = \frac{1}{n} \sum_i \left(- y_i \ln (f_\hat{\boldsymbol{\theta}}(\textbf{X}_i)) - (1 - y_i) \ln (1 - f_\hat{\boldsymbol{\theta}}(\textbf{X}_i) \right) \]</div>
<div class="math notranslate nohighlight">
\[ \hat{\boldsymbol{\theta}} = \displaystyle\arg \min_{\substack{\theta}} (\text{Average Cross-Entropy Loss}) \]</div>
</div>
<div class="section" id="statistical-justification-for-cross-entropy-loss">
<h2><span class="section-number">17.5.3. </span>Statistical justification for Cross-Entropy Loss<a class="headerlink" href="#statistical-justification-for-cross-entropy-loss" title="Permalink to this headline">¶</a></h2>
<p>The cross-entropy loss also has fundamental underpinnings in statistics. Since the logistic regression model predicts probabilities, given a particular logistic model we can ask, “What is the probability that this model produced the set of observed outcomes <span class="math notranslate nohighlight">\( \textbf{y} \)</span>?” We might naturally adjust the parameters of our model until the probability of drawing our dataset from the model is as high as possible. Although we will not prove it in this section, this procedure is equivalent to minimizing the cross-entropy loss—this is the <em>maximum likelihood</em> statistical justification for the cross-entropy loss.</p>
</div>
<div class="section" id="summary">
<h2><span class="section-number">17.5.4. </span>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>Average KL divergence can be interpreted as the average log difference between the two distributions <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(\hat{P_\boldsymbol{\theta}}\)</span> weighted by <span class="math notranslate nohighlight">\(P\)</span>. Minimizing average KL divergence also minimizes average cross-entropy loss. We can reduce the divergence of logistic regression models by selecting parameters that accurately classify commonly occurring data.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch/17"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="classification_log_reg.html" title="previous page"><span class="section-number">17.4. </span>Using Logistic Regression</a>
    <a class='right-next' id="next-link" href="classification_sgd.html" title="next page"><span class="section-number">17.6. </span>Fitting a Logistic Model</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Sam Lau, Joey Gonzalez, and Deb Nolan<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            <p>
License: CC BY-NC-ND 4.0
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-113006011-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>