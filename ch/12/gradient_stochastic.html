
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>12.4. Stochastic Gradient Descent &#8212; Principles and Techniques of Data Science</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.2d2078699c18a0efb88233928e1cf6ed.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.be0a4a0c39cd630af62a2fcf693f3f06.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="13. Probability and Generalization" href="../13/prob_and_gen.html" />
    <link rel="prev" title="12.3. Convexity" href="gradient_convexity.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  
  <h1 class="site-logo" id="site-title">Principles and Techniques of Data Science</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Frontmatter
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../prereqs.html">
   Prerequisites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation.html">
   Notation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Chapters
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../01/lifecycle_intro.html">
   1. The Data Science Lifecycle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02/design_intro.html">
   2. Data Design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../03/pandas_intro.html">
   3. Working with Tabular Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04/eda_intro.html">
   4. Exploratory Data Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05/cleaning_intro.html">
   5. Data Cleaning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../06/viz_intro.html">
   6. Data Visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../07/web_intro.html">
   7. Web Technologies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../08/text_intro.html">
   8. Working with Text
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../09/sql_intro.html">
   9. Relational Databases and SQL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../10/pca_intro.html">
   10. Dimensionality Reduction and PCA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../11/modeling_intro.html">
   11. Modeling and Estimation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="gradient_descent.html">
   12. Gradient Descent and Numerical Optimization
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="gradient_basics.html">
     12.1. Loss Minimization Using a Program
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gradient_descent_define.html">
     12.2. Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gradient_convexity.html">
     12.3. Convexity
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     12.4. Stochastic Gradient Descent
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../13/prob_and_gen.html">
   13. Probability and Generalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../14/linear_models.html">
   14. Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../15/feature_engineering.html">
   15. Feature Engineering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../16/bias_intro.html">
   16. The Bias-Variance Tradeoff
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../17/reg_intro.html">
   17. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../18/classification_intro.html">
   18. Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../19/hyp_intro.html">
   19. Statistical Inference
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendices
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../20/vector_space_review.html">
   Vector Space Review
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../21/ref_intro.html">
   Reference Tables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../22/contributors.html">
   Contributors
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/ch/12/gradient_stochastic.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/ch/12/gradient_stochastic.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   12.4.1. Stochastic Gradient Descent
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-the-mse-loss">
     12.4.1.1. Using the MSE Loss
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#behavior-of-stochastic-gradient-descent">
   12.4.2. Behavior of Stochastic Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-a-function-for-stochastic-gradient-descent">
   12.4.3. Defining a Function for Stochastic Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mini-batch-gradient-descent">
   12.4.4. Mini-batch Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-a-function-for-mini-batch-gradient-descent">
   12.4.5. Defining a Function for Mini-Batch Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   12.4.6. Summary
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell tag_hide_input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="c1"># Ignore numpy dtype warnings. These warnings are caused by an interaction</span>
<span class="c1"># between numpy and Cython and can be safely ignored.</span>
<span class="c1"># Reference: https://stackoverflow.com/a/40846742</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;numpy.dtype size changed&quot;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;numpy.ufunc size changed&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span><span class="p">,</span> <span class="n">interact_manual</span>
<span class="kn">import</span> <span class="nn">nbinteract</span> <span class="k">as</span> <span class="nn">nbi</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;talk&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_rows</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_columns</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># This option stops scientific notation for pandas</span>
<span class="c1"># pd.set_option(&#39;display.float_format&#39;, &#39;{:.2f}&#39;.format)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="stochastic-gradient-descent">
<h1><span class="section-number">12.4. </span>Stochastic Gradient Descent<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this headline">¶</a></h1>
<p>In this section, we discuss a modification to gradient descent that makes it much more useful for large datasets. The modified algorithm is called <strong>stochastic gradient descent</strong>.</p>
<p>Recall gradient descent updates our model parameter <span class="math notranslate nohighlight">\( \theta \)</span> by using the gradient of our chosen loss function. Specifically, we used this gradient update formula:</p>
<div class="math notranslate nohighlight">
\[
{\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_{\theta} L(\theta^{(t)}, \textbf{y})
\]</div>
<p>In this equation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta^{(t)}\)</span> is our current estimate of <span class="math notranslate nohighlight">\(\theta^*\)</span> at the <span class="math notranslate nohighlight">\(t\)</span>th iteration</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla_{\theta} L(\theta^{(t)}, \textbf{y})\)</span> is the gradient of the loss function</p></li>
<li><p>We compute the next estimate <span class="math notranslate nohighlight">\(\theta^{(t+1)}\)</span> by subtracting the product of <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\nabla_{\theta} L(\theta, \textbf{y})\)</span> computed at <span class="math notranslate nohighlight">\(\theta^{(t)}\)</span></p></li>
</ul>
<p><strong>Limitations of Batch Gradient Descent</strong></p>
<p>In the expression above, we calculate <span class="math notranslate nohighlight">\(\nabla_{\theta}L(\theta, \textbf{y})\)</span> using the average gradient of the loss function <span class="math notranslate nohighlight">\(\ell(\theta, y_i)\)</span> using <strong>the entire dataset</strong>. In other words, each time we update <span class="math notranslate nohighlight">\( \theta \)</span> we consult all the other points in our dataset as a complete batch. For this reason, the gradient update rule above is often referred to as <strong>batch gradient descent</strong>.</p>
<p>Unfortunately, we often work with large datasets. Although batch gradient descent will often find an optimal <span class="math notranslate nohighlight">\( \theta \)</span> in relatively few iterations, each iteration will take a long time to compute if the training set contains many points.</p>
<div class="section" id="id1">
<h2><span class="section-number">12.4.1. </span>Stochastic Gradient Descent<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>To circumvent the difficulty of computing a gradient across the entire training set, stochastic gradient descent approximates the overall gradient using <strong>a single randomly chosen data point</strong>. Since the observation is chosen randomly, we expect that using the gradient at each individual observation will eventually converge to the same parameters as batch gradient descent.</p>
<p>Consider once again the formula for batch gradient descent:</p>
<div class="math notranslate nohighlight">
\[
{\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_{\theta} L(\theta^{(t)}, \textbf{y})
\]</div>
<p>In this formula, we have the term <span class="math notranslate nohighlight">\(\nabla_{\theta} L(\theta^{(t)}, \textbf{y})\)</span>, the average gradient of the loss function across all points in the training set. That is:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\nabla_{\theta} L(\theta^{(t)}, \textbf{y}) &amp;= \frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta} \ell(\theta^{(t)}, y_i)
\end{aligned}
\]</div>
<p>Where <span class="math notranslate nohighlight">\( \ell(\theta, y_i) \)</span> is the loss at a single point in the training set. To conduct stochastic gradient descent, we simply replace the average gradient with the gradient at a single point. The gradient update formula for stochastic gradient descent is:</p>
<div class="math notranslate nohighlight">
\[
{\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_{\theta} \ell(\theta^{(t)}, y_i)
\]</div>
<p>In this formula, <span class="math notranslate nohighlight">\( y_i \)</span> is chosen randomly from <span class="math notranslate nohighlight">\( \textbf{y} \)</span>. Note that choosing the points randomly is critical to the success of stochastic gradient descent! If the points are not chosen randomly, stochastic gradient descent may produce significantly worse results than batch gradient descent.</p>
<p>We most commonly run stochastic gradient descent by shuffling the data points and using each one in its shuffled order until one complete pass through the training data is completed. If the algorithm hasn’t converged, we reshuffle the points and run another pass through the data. Each <strong>iteration</strong> of stochastic gradient descent looks at one data point; each complete pass through the data is called an <strong>epoch</strong>.</p>
<div class="section" id="using-the-mse-loss">
<h3><span class="section-number">12.4.1.1. </span>Using the MSE Loss<a class="headerlink" href="#using-the-mse-loss" title="Permalink to this headline">¶</a></h3>
<p>As an example, we derive the stochastic gradient descent update formula for the mean squared loss. Recall the definition of the mean squared loss:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
L(\theta, \textbf{y})
&amp;= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2
\end{aligned}
\]</div>
<p>Taking the gradient with respect to <span class="math notranslate nohighlight">\( \theta \)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\nabla_{\theta}  L(\theta, \textbf{y})
&amp;= \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - \theta)
\end{aligned}
\]</div>
<p>Since the above equation gives us the average gradient loss across all points in the dataset, the gradient loss on a single point is simply the piece of the equation that is being averaged:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\nabla_{\theta}  \ell(\theta, y_i)
&amp;= -2(y_i - \theta)
\end{aligned}
\]</div>
<p>Thus, the batch gradient update rule for the MSE loss is:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
{\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \left( \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - \theta) \right)
\end{aligned}
\]</div>
<p>And the stochastic gradient update rule is:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
{\theta}^{(t+1)} = \theta^{(t)} - \alpha \cdot \left( -2(y_i - \theta) \right)
\end{aligned}
\]</div>
</div>
</div>
<div class="section" id="behavior-of-stochastic-gradient-descent">
<h2><span class="section-number">12.4.2. </span>Behavior of Stochastic Gradient Descent<a class="headerlink" href="#behavior-of-stochastic-gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>Since stochastic descent only examines a single data point a time, it will likely update <span class="math notranslate nohighlight">\( \theta \)</span> less accurately than a update from batch gradient descent. However, since stochastic gradient descent computes updates much faster than batch gradient descent, stochastic gradient descent can make significant progress towards the optimal <span class="math notranslate nohighlight">\( \theta \)</span> by the time batch gradient descent finishes a single update.</p>
<p>In the image below, we show successive updates to <span class="math notranslate nohighlight">\( \theta \)</span> using batch gradient descent. The darkest area of the plot corresponds to the optimal value of <span class="math notranslate nohighlight">\( \theta \)</span> on our training data, <span class="math notranslate nohighlight">\( \hat{\theta} \)</span>.</p>
<p>(This image technically shows a model that has two parameters, but it is more important to see that batch gradient descent always takes a step towards <span class="math notranslate nohighlight">\( \hat{\theta} \)</span>.)</p>
<a class="reference internal image-reference" href="../../_images/gradient_stochastic_gd.png"><img alt="gradient_stochastic_gd.png" class="align-center" src="../../_images/gradient_stochastic_gd.png" style="width: 500px;" /></a>
<p>Stochastic gradient descent, on the other hand, often takes steps away from <span class="math notranslate nohighlight">\( \hat{\theta} \)</span>! However, since it makes updates more often, it often converges faster than batch gradient descent.</p>
<a class="reference internal image-reference" href="../../_images/gradient_stochastic_sgd.png"><img alt="gradient_stochastic_sgd.png" class="align-center" src="../../_images/gradient_stochastic_sgd.png" style="width: 500px;" /></a>
</div>
<div class="section" id="defining-a-function-for-stochastic-gradient-descent">
<h2><span class="section-number">12.4.3. </span>Defining a Function for Stochastic Gradient Descent<a class="headerlink" href="#defining-a-function-for-stochastic-gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>As we previously did for batch gradient descent, we define a function that computes the stochastic gradient descent of the loss function. It will be similar to our <code class="docutils literal notranslate"><span class="pre">minimize</span></code> function but we will need to implement the random selection of one observation at each iteration.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">minimize_sgd</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_loss_fn</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Uses stochastic gradient descent to minimize loss_fn.</span>
<span class="sd">    Returns the minimizing value of theta once theta changes</span>
<span class="sd">    less than 0.001 between iterations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">NUM_OBS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">NUM_OBS</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">rand_obs</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_loss_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">rand_obs</span><span class="p">)</span>
            <span class="n">new_theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">gradient</span>
        
            <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">new_theta</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.001</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">new_theta</span>
        
            <span class="n">theta</span> <span class="o">=</span> <span class="n">new_theta</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="mini-batch-gradient-descent">
<h2><span class="section-number">12.4.4. </span>Mini-batch Gradient Descent<a class="headerlink" href="#mini-batch-gradient-descent" title="Permalink to this headline">¶</a></h2>
<p><strong>Mini-batch gradient descent</strong> strikes a balance between batch gradient descent and stochastic gradient descent by increasing the number of observations that we select at each iteration. In mini-batch gradient descent, we use a few data points for each gradient update instead of a single point.</p>
<p>We use the average of the gradients of their loss functions to construct an estimate of the true gradient of the cross entropy loss. If <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is the mini-batch of data points that we randomly sample from the <span class="math notranslate nohighlight">\(n\)</span> observations, the following approximation holds.</p>
<div class="math notranslate nohighlight">
\[
\nabla_\theta L(\theta, \textbf{y}) \approx \frac{1}{|\mathcal{B}|} \sum_{i\in\mathcal{B}}\nabla_{\theta}\ell(\theta, y_i)
\]</div>
<p>As with stochastic gradient descent, we perform mini-batch gradient descent by shuffling our training data and selecting mini-batches by iterating through the shuffled data. After each epoch, we re-shuffle our data and select new mini-batches.</p>
<p>While we have made the distinction between stochastic and mini-batch gradient descent in this textbook, stochastic gradient descent is sometimes used as an umbrella term that encompasses the selection of a mini-batch of any size.</p>
<p><strong>Selecting the Mini-Batch Size</strong></p>
<p>Mini-batch gradient descent is most optimal when running on a Graphical Processing Unit (GPU) chip found in some computers. Since computations on these types of hardware can be executed in parallel, using a mini-batch can increase the accuracy of the gradient without increasing computation time. Depending on the memory of the GPU, the mini-batch size is often set between 10 and 100 observations.</p>
</div>
<div class="section" id="defining-a-function-for-mini-batch-gradient-descent">
<h2><span class="section-number">12.4.5. </span>Defining a Function for Mini-Batch Gradient Descent<a class="headerlink" href="#defining-a-function-for-mini-batch-gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>A function for mini-batch gradient descent requires the ability to select a batch size. Below is a function that implements this feature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">minimize_mini_batch</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_loss_fn</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Uses mini-batch gradient descent to minimize loss_fn.</span>
<span class="sd">    Returns the minimizing value of theta once theta changes</span>
<span class="sd">    less than 0.001 between iterations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">NUM_OBS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">minibatch_size</span> <span class="o">&lt;</span> <span class="n">NUM_OBS</span>
    
    <span class="n">theta</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">NUM_OBS</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="p">):</span>
            <span class="n">mini_batch</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">minibatch_size</span><span class="p">]</span>
            <span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_loss_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">mini_batch</span><span class="p">)</span>
            <span class="n">new_theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">gradient</span>
            
            <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">new_theta</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.001</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">new_theta</span>
            
            <span class="n">theta</span> <span class="o">=</span> <span class="n">new_theta</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="summary">
<h2><span class="section-number">12.4.6. </span>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>We use batch gradient descent to iteratively improve model parameters until the model achieves minimal loss. Since batch gradient descent is computationally intractable with large datasets, we often use stochastic gradient descent to fit models instead. When using a GPU, mini-batch gradient descent can converge more quickly than stochastic gradient descent for the same computational cost. For large datasets, stochastic gradient descent and mini-batch gradient descent are often preferred to batch gradient descent for their faster computation times.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch/12"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="gradient_convexity.html" title="previous page"><span class="section-number">12.3. </span>Convexity</a>
    <a class='right-next' id="next-link" href="../13/prob_and_gen.html" title="next page"><span class="section-number">13. </span>Probability and Generalization</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Sam Lau, Joey Gonzalez, and Deb Nolan<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            <p>
License: CC BY-NC-ND 4.0
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-113006011-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>