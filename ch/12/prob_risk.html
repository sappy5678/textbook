
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>12.3. Risk &#8212; Principles and Techniques of Data Science</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="13. Linear Models" href="../13/linear_models.html" />
    <link rel="prev" title="12.2. Expectation and Variance" href="prob_exp_var.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  
  <h1 class="site-logo" id="site-title">Principles and Techniques of Data Science</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Frontmatter
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../prereqs.html">
   Prerequisites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation.html">
   Notation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapters
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../01/lifecycle_intro.html">
   1. The Data Science Lifecycle
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../01/lifecycle_students_1.html">
     1.1. The Students of Data 100
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/lifecycle_students_2.html">
     1.2. Exploratory Data Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/lifecycle_students_3.html">
     1.3. What’s in a Name?
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../02/design_intro.html">
   2. Data Design
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../02/design_dewey_truman.html">
     2.1. Dewey Defeats Truman
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/design_prob_overview.html">
     2.2. Probability Overview
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/design_sampling.html">
     2.3. Probability Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/design_srs_vs_big_data.html">
     2.4. SRS vs. “Big Data”
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../03/pandas_intro.html">
   3. Working with Tabular Data
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../03/pandas_structure.html">
     3.1. Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/pandas_indexes.html">
     3.2. Indexes, Slicing, and Sorting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/pandas_grouping_pivoting.html">
     3.3. Grouping and Pivoting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/pandas_apply_strings_plotting.html">
     3.4. Apply, Strings, and Plotting
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../04/eda_intro.html">
   4. Exploratory Data Analysis
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../04/eda_data_types.html">
     4.1. Data Types
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../05/cleaning_intro.html">
   5. Data Cleaning
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../05/cleaning_calls.html">
     5.1. Cleaning the Calls Dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/cleaning_stops.html">
     5.2. Cleaning The Stops Dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/cleaning_structure.html">
     5.3. Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/cleaning_granularity.html">
     5.4. Granularity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/cleaning_scope.html">
     5.5. Scope
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/cleaning_temp.html">
     5.6. Temporality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/cleaning_faithfulness.html">
     5.7. Faithfulness
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../06/viz_intro.html">
   6. Data Visualization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../06/viz_quantitative.html">
     6.1. Visualizing Quantitative Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/viz_qualitative.html">
     6.2. Visualizing Qualitative Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/viz_matplotlib.html">
     6.3. Customizing Plots using
     <code class="docutils literal notranslate">
      <span class="pre">
       matplotlib
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/viz_principles.html">
     6.4. Visualization Principles
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/viz_principles_2.html">
     6.5. Visualization Principles Continued
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/viz_philosophy.html">
     6.6. Philosophy for Data Visualization
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../07/web_intro.html">
   7. Web Technologies
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../07/web_http.html">
     7.1. HTTP
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../08/text_intro.html">
   8. Working with Text
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../08/text_strings.html">
     8.1. Python String Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08/text_regex.html">
     8.2. Regular Expressions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08/text_re.html">
     8.3. Regex and Python
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../09/sql_intro.html">
   9. Relational Databases and SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../09/sql_rdbms.html">
     9.1. The Relational Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/sql_basics.html">
     9.2. SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/sql_joins.html">
     9.3. SQL Joins
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../10/modeling_intro.html">
   10. Modeling and Estimation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../10/modeling_simple.html">
     10.1. Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/modeling_loss_functions.html">
     10.2. Loss Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/modeling_abs_huber.html">
     10.3. Absolute and Huber Loss
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11/gradient_descent.html">
   11. Gradient Descent and Numerical Optimization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11/gradient_basics.html">
     11.1. Loss Minimization Using a Program
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11/gradient_descent_define.html">
     11.2. Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11/gradient_convexity.html">
     11.3. Convexity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11/gradient_stochastic.html">
     11.4. Stochastic Gradient Descent
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="prob_and_gen.html">
   12. Probability and Generalization
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="prob_random_vars.html">
     12.1. Random Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prob_exp_var.html">
     12.2. Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     12.3. Risk
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13/linear_models.html">
   13. Linear Models
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13/linear_tips.html">
     13.1. Predicting Tip Amounts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13/linear_grad.html">
     13.2. Fitting a Linear Model Using Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13/linear_multiple.html">
     13.3. Multiple Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13/linear_projection.html">
     13.4. Least Squares — A Geometric Perspective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13/linear_case_study.html">
     13.5. Linear Regression Case Study
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14/feature_engineering.html">
   14. Feature Engineering
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14/feature_one_hot.html">
     14.1. The Walmart dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14/feature_polynomial.html">
     14.2. Predicting Ice Cream Ratings
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../15/bias_intro.html">
   15. The Bias-Variance Tradeoff
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../15/bias_risk.html">
     15.1. Risk and Loss Minimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15/bias_modeling.html">
     15.2. Model Bias and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15/bias_cv.html">
     15.3. Cross-Validation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../16/reg_intro.html">
   16. Regularization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../16/reg_intuition.html">
     16.1. Regularization Intuition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16/reg_ridge.html">
     16.2. L2 Regularization: Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16/reg_lasso.html">
     16.3. L1 Regularization: Lasso Regression
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17/classification_intro.html">
   17. Classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17/classification_prob.html">
     17.1. Regression on Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17/classification_log_model.html">
     17.2. The Logistic Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17/classification_cost.html">
     17.3. A Loss Function for the Logistic Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17/classification_log_reg.html">
     17.4. Using Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17/classification_cost_justification.html">
     17.5. Approximating the Empirical Probability Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17/classification_sgd.html">
     17.6. Fitting a Logistic Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17/classification_sensitivity_specificity.html">
     17.7. Evaluating Logistic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17/classification_multiclass.html">
     17.8. Multiclass Classification
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../18/hyp_intro.html">
   18. Statistical Inference
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../18/hyp_introduction.html">
     18.1. Hypothesis Testing and Confidence Intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18/hyp_introduction_part2.html">
     18.2. Permutation Test
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18/hyp_regression.html">
     18.3. Bootstrapping for Linear Regression (Inference for the True Coefficients)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18/hyp_studentized.html">
     18.4. The Studentized Bootstrap
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18/hyp_phacking.html">
     18.5. P-hacking
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19/pca_intro.html">
   19. Dimensionality Reduction and PCA
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19/pca_dims.html">
     19.1. Dimensions of a Data Table
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19/pca_svd.html">
     19.2. PCA using the Singular Value Decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19/pca_in_practice.html">
     19.3. PCA in Practice
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendices
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../20/vector_space_review.html">
   Vector Space Review
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21/ref_intro.html">
   Reference Tables
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21/ref_pandas.html">
     pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21/ref_seaborn.html">
     Seaborn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21/ref_matplotlib.html">
     matplotlib
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21/ref_sklearn.html">
     scikit-learn
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../22/contributors.html">
   Contributors
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/ch/12/prob_risk.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/ch/12/prob_risk.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definition">
   12.3.1. Definition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#minimizing-the-risk">
   12.3.2. Minimizing the Risk
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#analysis-of-risk">
     12.3.2.1. Analysis of Risk
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#empirical-risk-minimization">
   12.3.3. Empirical Risk Minimization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-importance-of-random-sampling">
     12.3.3.1. The Importance of Random Sampling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#connection-to-loss-minimization">
     12.3.3.2. Connection to Loss Minimization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   12.3.4. Summary
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="risk">
<h1><span class="section-number">12.3. </span>Risk<a class="headerlink" href="#risk" title="Permalink to this headline">¶</a></h1>
<p>In a modeling scenario presented in a previous chapter, a waiter collected a dataset of tips for a particular month of work. We selected a constant model and minimized the mean squared error (MSE) loss function on this dataset, guaranteeing that our constant model outperforms all other constant models on this dataset and loss function. The constant model has a single parameter, <span class="math notranslate nohighlight">\( \theta \)</span>. We found that the optimizing parameter <span class="math notranslate nohighlight">\( \hat{\theta} = \text{mean}(\textbf y) \)</span> for the MSE loss.</p>
<p>Although such a model makes relatively accurate predictions on its training data, we would like to know whether the model will perform well on new data from the population. To represent this notion, we introduce statistical <strong>risk</strong>, also known as the <strong>expected loss</strong>.</p>
<div class="section" id="definition">
<h2><span class="section-number">12.3.1. </span>Definition<a class="headerlink" href="#definition" title="Permalink to this headline">¶</a></h2>
<p>A model’s risk is the expected value of the model’s loss on randomly chosen points from the population.</p>
<p>In this scenario, the population consists of all tip percentages our waiter receives during his employment, including future tips. We use the random variable <span class="math notranslate nohighlight">\( X \)</span> to represent a randomly chosen tip percent from the population, and the usual variable <span class="math notranslate nohighlight">\( \theta \)</span> to represent the constant model’s prediction. Using this notation, the risk <span class="math notranslate nohighlight">\( R(\theta) \)</span> of our model is:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
R(\theta) = \mathbb{E}\left[(X - \theta)^2\right]
\end{aligned}
\]</div>
<p>In the expression above, we use the MSE loss which gives the inner <span class="math notranslate nohighlight">\( (X - \theta)^2 \)</span> in the expectation. The risk is a function of <span class="math notranslate nohighlight">\( \theta \)</span> since we can change <span class="math notranslate nohighlight">\( \theta \)</span> as we please.</p>
<p>Unlike loss alone, using risk allows us to reason about the model’s accuracy on the population in general. If our model achieves a low risk, our model will make accurate predictions on points from the population in the long term. On the other hand, if our model has a high risk it will in general perform poorly on data from the population.</p>
<p>Naturally, we would like to choose the value of <span class="math notranslate nohighlight">\( \theta \)</span> that makes the model’s risk as low as possible. We use the variable <span class="math notranslate nohighlight">\( \theta^* \)</span> to represent the risk-minimizing value of <span class="math notranslate nohighlight">\( \theta \)</span>, or the optimal model parameter for the population. To clarify, <span class="math notranslate nohighlight">\( \theta^* \)</span> represents the model parameter that minimizes risk while <span class="math notranslate nohighlight">\( \hat{\theta} \)</span> represents the parameter that minimizes dataset-specific loss.</p>
</div>
<div class="section" id="minimizing-the-risk">
<h2><span class="section-number">12.3.2. </span>Minimizing the Risk<a class="headerlink" href="#minimizing-the-risk" title="Permalink to this headline">¶</a></h2>
<p>Let’s find the value of <span class="math notranslate nohighlight">\( \theta \)</span> that minimizes the risk. Previously, we used calculus to perform this minimization. This time, we will use a mathematical trick that produces a meaningful final expression. We replace <span class="math notranslate nohighlight">\(X - \theta\)</span> with <span class="math notranslate nohighlight">\(X - \mathbb{E}[X] + \mathbb{E}[X] - \theta\)</span> and expand:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
R(\theta) &amp;=  \mathbb{E}[(X - \theta)^2] \\
&amp;= \mathbb{E}\left[
  (X - \mathbb{E}[X] + \mathbb{E}[X] - \theta)^2
\right] \\
&amp;= \mathbb{E}\left[
  \bigl( (X - \mathbb{E}[X]) + (\mathbb{E}[X] - \theta) \bigr)^2
\right] \\
&amp;= \mathbb{E}\left[
  (X - \mathbb{E}[X])^2 + 2(X - \mathbb{E}[X])(\mathbb{E}[X] - \theta) + (\mathbb{E}[X]- \theta)^2
\right] \\
\end{aligned}
\end{split}\]</div>
<p>Now, we apply the linearity of expectation and simplify. We use the identity <span class="math notranslate nohighlight">\( \mathbb{E}\left[ (X - \mathbb{E}[X]) \right] = 0 \)</span> which is roughly equivalent to stating that <span class="math notranslate nohighlight">\( \mathbb{E}[X] \)</span> lies at the center of the distribution of <span class="math notranslate nohighlight">\( X \)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
R(\theta) &amp;=
  \mathbb{E}\left[ (X - \mathbb{E}[X])^2 \right]
  + \mathbb{E}\left[ 2(X - \mathbb{E}[X])(\mathbb{E}[X] - \theta) \right]
  + \mathbb{E}\left[ (\mathbb{E}[X]- \theta)^2 \right] \\
&amp;=
  \mathbb{E}\left[ (X - \mathbb{E}[X])^2 \right]
  + 2 (\mathbb{E}[X] - \theta) \underbrace{ \mathbb{E}\left[ (X - \mathbb{E}[X]) \right]}_{= 0}
  + (\mathbb{E}[X]- \theta)^2 \\
&amp;=
  \mathbb{E}\left[ (X - \mathbb{E}[X])^2 \right]
  + 0
  + (\mathbb{E}[X]- \theta)^2 \\
R(\theta) &amp;=
  \mathbb{E}\left[ (X - \mathbb{E}[X])^2 \right]
  + (\mathbb{E}[X]- \theta)^2 \\
\end{aligned}
\end{split}\]</div>
<p>Notice that the first term in the expression above is the <strong>variance</strong> of <span class="math notranslate nohighlight">\( X \)</span>, <span class="math notranslate nohighlight">\( Var(X) \)</span>, which has no dependence on <span class="math notranslate nohighlight">\( \theta \)</span>. The second term gives a measure of how close <span class="math notranslate nohighlight">\( \theta \)</span> is to <span class="math notranslate nohighlight">\( \mathbb{E}[X] \)</span>. Because of this, the second term is called the <strong>bias</strong> of our model. In other words, the model’s risk is the bias of the model plus the variance of the quantity we are trying to predict:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
R(\theta) &amp;=
  \underbrace{(\mathbb{E}[X]- \theta)^2}_\text{bias}
  + \underbrace{Var(X)}_\text{variance}
\end{aligned}
\]</div>
<p>Thus, the risk is minimized when our model has no bias: <span class="math notranslate nohighlight">\( \theta^* =  \mathbb{E}[X] \)</span> .</p>
<div class="section" id="analysis-of-risk">
<h3><span class="section-number">12.3.2.1. </span>Analysis of Risk<a class="headerlink" href="#analysis-of-risk" title="Permalink to this headline">¶</a></h3>
<p>Notice that when our model has no bias, the risk is usually a positive quantity. This implies that even an optimal model will have prediction error. Intuitively, this occurs because a constant model will only predict a single number while <span class="math notranslate nohighlight">\( X \)</span> may take on any value from the population. The variance term captures the magnitude of the error. A low variance means that <span class="math notranslate nohighlight">\( X \)</span> will likely take a value close to <span class="math notranslate nohighlight">\( \theta \)</span>, whereas a high variance means that <span class="math notranslate nohighlight">\( X \)</span> is more likely to take on a value far from <span class="math notranslate nohighlight">\( \theta \)</span>.</p>
</div>
</div>
<div class="section" id="empirical-risk-minimization">
<h2><span class="section-number">12.3.3. </span>Empirical Risk Minimization<a class="headerlink" href="#empirical-risk-minimization" title="Permalink to this headline">¶</a></h2>
<p>From the above analysis, we would like to set <span class="math notranslate nohighlight">\( \theta = \mathbb{E}[X] \)</span>. Unfortunately, calculating <span class="math notranslate nohighlight">\( \mathbb{E}[X] \)</span> requires complete knowledge of the population. To understand why, examine the expression for <span class="math notranslate nohighlight">\( \mathbb{E}[X] \)</span>:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbb{E}[X] = \sum_{x \in \mathbb{X}} x \cdot P(X = x)
\end{aligned}
\]</div>
<p><span class="math notranslate nohighlight">\( P(X = x) \)</span> represents the probability that <span class="math notranslate nohighlight">\( X \)</span> takes on a specific value from the population. To calculate this probability, however, we need to know all possible values of <span class="math notranslate nohighlight">\( X \)</span> and how often they appear in the population. In other words, to perfectly minimize a model’s risk on a population, we need access to the population.</p>
<p>We can tackle this issue by remembering that the distribution of values in a large random sample will be close to the distribution of values in the population. If this is true about our sample, we can treat the sample as though it were the population itself.</p>
<p>Suppose we draw points at random from the sample instead of the population. Since there are <span class="math notranslate nohighlight">\( n \)</span> total points in the sample <span class="math notranslate nohighlight">\( \mathbf{x} = \{ x_1, x_2, \ldots, x_n \} \)</span>, each point <span class="math notranslate nohighlight">\( x_i \)</span> has probability <span class="math notranslate nohighlight">\( \frac{1}{n} \)</span> of appearing. Now we can create an approximation for <span class="math notranslate nohighlight">\( \mathbb{E}[X] \)</span>:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbb{E}[X]
&amp;\approx \frac{1}{n} \sum_{i=1}^n x_i = \text{mean}({\mathbf{x}})
\end{aligned}
\]</div>
<p>Thus, our best estimate of <span class="math notranslate nohighlight">\( \theta^* \)</span> using the information captured in a random sample is <span class="math notranslate nohighlight">\( \hat{\theta} = \text{mean}(\mathbf{x}) \)</span>. We say that <span class="math notranslate nohighlight">\( \hat{\theta} \)</span> minimizes the <strong>empirical risk</strong>, the risk calculated using the sample as a stand-in for the population.</p>
<div class="section" id="the-importance-of-random-sampling">
<h3><span class="section-number">12.3.3.1. </span>The Importance of Random Sampling<a class="headerlink" href="#the-importance-of-random-sampling" title="Permalink to this headline">¶</a></h3>
<p>It is essential to note the importance of random sampling in the approximation above. If our sample is non-random, we cannot make the above assumption that the sample’s distribution is similar to the population’s. Using a non-random sample to estimate <span class="math notranslate nohighlight">\( \theta^* \)</span> will often result in a biased estimation and a higher risk.</p>
</div>
<div class="section" id="connection-to-loss-minimization">
<h3><span class="section-number">12.3.3.2. </span>Connection to Loss Minimization<a class="headerlink" href="#connection-to-loss-minimization" title="Permalink to this headline">¶</a></h3>
<p>Recall that we have previously shown <span class="math notranslate nohighlight">\( \hat{\theta} = \text{mean}(\mathbf{x}) \)</span> minimizes the MSE loss on a dataset. Now, we have taken a meaningful step forward. If our training data are a random sample, <span class="math notranslate nohighlight">\( \hat{\theta} = \text{mean}(\mathbf{x}) \)</span> not only produces the best model for its training data but also produces the best model for the population given the information we have in our sample.</p>
</div>
</div>
<div class="section" id="summary">
<h2><span class="section-number">12.3.4. </span>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>Using the mathematical tools developed in this chapter, we have developed an understanding of our model’s performance on the population. A model makes accurate predictions if it minimizes <strong>statistical risk</strong>. We found that the globally optimal model parameter is:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\theta^* =  \mathbb{E}[X]
\end{aligned}
\]</div>
<p>Since we cannot readily compute this, we found the model parameter that minimizes the <strong>empirical risk</strong>.</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\hat \theta = \text{mean}(\mathbf x)
\end{aligned}
\]</div>
<p>If the training data are randomly sampled from the population, it is likely that <span class="math notranslate nohighlight">\( \hat{\theta} \approx \theta^* \)</span>. Thus, a constant model trained on a large random sample from the population will likely perform well on the population as well.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch/12"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="prob_exp_var.html" title="previous page"><span class="section-number">12.2. </span>Expectation and Variance</a>
    <a class='right-next' id="next-link" href="../13/linear_models.html" title="next page"><span class="section-number">13. </span>Linear Models</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Sam Lau, Joey Gonzalez, and Deb Nolan<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            <p>
License: CC BY-NC-ND 4.0
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-113006011-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>